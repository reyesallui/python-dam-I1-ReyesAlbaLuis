# worldChef.py
"""
Unifica utilidades de procesamiento de texto con:
- Normalizaci√≥n
- B√∫squeda de patrones con RE
- Resumen simple
- Extracci√≥n NER
- Palabras clave
- An√°lisis de sentimiento
"""


import re
import sys
import os
from collections import Counter
from datetime import datetime


# ----------------------
# Intentos de import
# ----------------------
# Trata de importar spaCy; si no est√°, asigna None para manejo posterior
try:
    import spacy
except ImportError:
    spacy = None

# Trata de importar nltk y m√≥dulos necesarios; si no est√°, asigna None
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
except ImportError:
    nltk = None

# Trata de importar pipeline desde transformers; si no est√°, asigna None
try:
    from transformers import pipeline
except ImportError:
    pipeline = None


# ----------------------
# Logging de sesi√≥n - Marius
# ----------------------
class SessionLogger:
    """
    Logger de sesi√≥n para guardar los resultados de los an√°lisis en un archivo.

    Cada vez que se crea una instancia de esta clase se genera un archivo
    nuevo en la carpeta `logs/` con nombre `session_YYYYMMDD_HHMMSS.log`.

    M√©todos principales:
    - log(tipo, entrada, resultado): guarda un an√°lisis gen√©rico.
    - registrar_patron(tipo_patron, coincidencias): guarda b√∫squedas por patr√≥n.

    El logger mantiene el archivo legible para un humano, con encabezados,
    timestamps y presentaci√≥n en vi√±etas para listas. Est√° pensado para
    seguimiento de sesiones interactivas desde el CLI.
    """
    # Constructor crea carpeta logs si no existe y crea fichero log con timestamp
    def __init__(self):
        if not os.path.exists("logs"):
            os.makedirs("logs")
            print("üìÅ Carpeta 'logs' creada autom√°ticamente.")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.filename = f"logs/session_{timestamp}.log"
        self._write_header()

    # Escribe cabecera inicial del log con formato y fecha
    def _write_header(self):
        with open(self.filename, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write(f"  SESI√ìN wordChef - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")

    # A√±ade entrada de log con tipo, fragmento de entrada y resultado
    def log(self, tipo: str, entrada: str, resultado):
        with open(self.filename, 'a', encoding='utf-8') as f:
            f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {tipo}\n")
            f.write("-"*80 + "\n")
            entrada_truncada = entrada[:100] + ('...' if len(entrada) > 100 else '')
            f.write(f"Entrada: {entrada_truncada}\n\n")
            if isinstance(resultado, dict):
                for clave, valor in resultado.items():
                    f.write(f"  {clave}:\n")
                    if isinstance(valor, (list, set)):
                        for item in valor:
                            f.write(f"    ‚Ä¢ {item}\n")
                    else:
                        f.write(f"   {valor}\n")
            else:
                f.write(f"Resultado: {resultado}\n")
            f.write("\n" + "="*80 + "\n\n")


logger = SessionLogger()
print(f"üìù Sesi√≥n iniciada. Logs guardados en: {logger.filename}\n")


# ----------------------
# Entrada de texto --Luis
# ----------------------
# Funci√≥n para leer texto desde un archivo; devuelve el contenido o None si error
def leer_archivo(ruta: str) -> str | None:
    if not os.path.exists(ruta):
        print(f"Error: El archivo '{ruta}' no existe.")
        return None
    try:
        with open(ruta, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"Error al leer el archivo: {e}")
        return None

# ----------------------
# Inicializaci√≥n de dependencias
# ----------------------

def cargar_modelo_spacy():
    """
    Carga un modelo de spaCy para procesamiento en espa√±ol.

    Intenta cargar varios modelos de spaCy en el siguiente orden:
    1. `es_core_news_sm`
    2. `es_core_news_md`
    3. `xx_sent_ud_sm`

    Si ninguno est√° disponible, crea un pipeline vac√≠o para espa√±ol (`spacy.blank("es")`)
    e intenta a√±adir un `sentencizer` para segmentaci√≥n en oraciones.

    Returns:
        nlp (spacy.lang): Objeto de procesamiento ling√º√≠stico de spaCy.
        Si spaCy no est√° instalado, retorna `None`.
    """
    if spacy is None:
        print("Aviso: spaCy no instalado. Algunas funciones no estar√°n disponibles.")
        return None

    modelos = ["es_core_news_sm", "es_core_news_md", "xx_sent_ud_sm"]

    for m in modelos:
        try:
            nlp = spacy.load(m)
            if "sentencizer" not in nlp.pipe_names:
                try:
                    nlp.add_pipe("sentencizer")
                except Exception:
                    pass
            return nlp
        except Exception:
            continue

    # Si no carg√≥ ning√∫n modelo, crear pipeline vac√≠o
    nlp = spacy.blank("es")
    try:
        nlp.add_pipe("sentencizer")
    except Exception:
        pass
    return nlp


def inicializar_nltk():
    """
    Inicializa recursos necesarios de NLTK.

    Verifica si los paquetes:
    - `punkt` (tokenizador)
    - `stopwords` (lista de stopwords)
    
    est√°n disponibles. Si no, los descarga autom√°ticamente.

    Este procedimiento solo se ejecuta si NLTK est√° instalado.

    Returns:
        None
    """
    if nltk is not None:
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
        except Exception:
            nltk.download('punkt')
            nltk.download('stopwords')


def inicializar_sentimiento():
    """
    Inicializa un clasificador de sentimiento usando Transformers.

    Carga el modelo `nlptown/bert-base-multilingual-uncased-sentiment`
    a trav√©s del pipeline de HuggingFace.

    Returns:
        pipeline or None:
            - Un pipeline de an√°lisis de sentimiento si Transformers est√° disponible.
            - `None` si no se puede inicializar o la librer√≠a no est√° instalada.
    """
    if pipeline is None:
        return None

    try:
        return pipeline(
            "sentiment-analysis",
            model="nlptown/bert-base-multilingual-uncased-sentiment"
        )
    except Exception:
        return None

# ----------------------
# Normalizaci√≥n
# ----------------------
# Diccionario para corregir errores comunes de palabras
CORRECCIONES_COMUNES = {
    "haiga": "haya", "naiden": "nadie", "nadien": "nadie",
    "aserca": "acerca", "enserio": "en serio", "haber": "a ver", "iva": "iba",
}
# Sustantivos que no deben ser neutros, con art√≠culo definido
SUSTANTIVOS_NO_NEUTROS = {
    "casa": "la casa", "persona": "la persona", "gente": "la gente",
    "ni√±o": "el ni√±o", "ni√±a": "la ni√±a", "camisa": "la camisa"
}

# Corrige palabras comunes y evita repeticiones consecutivas en un doc spaCy
def corregir_palabras(doc):
    """
    Corrige errores ortogr√°ficos comunes y evita repeticiones consecutivas.
    
    Args:
        doc: Documento spaCy procesado.
    
    Returns:
        str: Texto corregido y sin repeticiones.
    """
    
    # Si el documento es None, retornar cadena vac√≠a para evitar errores
    if doc is None:
        return ""
    
    corregido = []  # Lista donde se ir√°n guardando las palabras corregidas
    
    # Recorremos token por token del documento spaCy
    for i, token in enumerate(doc):
        palabra = token.text.lower()  # Normalizamos a min√∫sculas para comparar
        
        # Si la palabra est√° en el diccionario de correcciones comunes,
        # la reemplazamos por la versi√≥n corregida
        if palabra in CORRECCIONES_COMUNES:
            corregido.append(CORRECCIONES_COMUNES[palabra])
            continue  # Saltamos al siguiente token
        
        # Si la palabra est√° en el diccionario de sustantivos no neutros,
        # tambi√©n la reemplazamos
        if palabra in SUSTANTIVOS_NO_NEUTROS:
            corregido.append(SUSTANTIVOS_NO_NEUTROS[palabra])
            continue
        
        # Evitar repeticiones consecutivas:
        # Si no es el primer token y la palabra es igual a la anterior, la omitimos
        if i > 0 and palabra == doc[i-1].text.lower():
            continue
        
        # Si no aplica ninguna correcci√≥n, conservar la palabra original
        corregido.append(token.text)
    
    # Unimos la lista en un string final
    return " ".join(corregido)


# Normaliza el texto: lematiza, elimina repeticiones, corrige palabras, usando spaCy si est√°
def normalizador_texto(texto, nlp):
    """
    Normaliza texto: lematiza, elimina repeticiones y corrige palabras.
    
    Args:
        texto (str): Texto original a procesar.
        nlp: Pipeline spaCy para lematizaci√≥n.
    
    Returns:
        dict: {original, lematizado, sin_repeticiones, corregido}
    """
    
    # Si el texto est√° vac√≠o o solo tiene espacios, retornar None
    if not texto or len(texto.strip()) == 0:
        return None
    
    # Si spaCy no est√° disponible, procesamos √∫nicamente lo posible sin √©l
    if nlp is None:
        palabras = texto.split()  # Separamos en palabras "a mano"
        
        # Eliminamos repeticiones consecutivas (case-insensitive)
        sin_repeticiones = " ".join([
            palabras[i] for i in range(len(palabras))
            if i == 0 or palabras[i].lower() != palabras[i-1].lower()
        ])
        
        # Retornamos lo que se puede sin spaCy
        return {
            "original": texto,
            "lematizado": "(spaCy requerido)",     # No disponible
            "sin_repeticiones": sin_repeticiones,
            "corregido": "(spaCy requerido)"        # No disponible
        }
    
    # Si spaCy s√≠ est√° disponible, procesamos el texto completo
    doc = nlp(texto)
    
    # Lematizamos usando spaCy
    lematizado = " ".join([t.lemma_ for t in doc])
    
    # Eliminamos repeticiones consecutivas del texto original
    palabras = texto.split()
    sin_repeticiones = " ".join([
        palabras[i] for i in range(len(palabras))
        if i == 0 or palabras[i].lower() != palabras[i-1].lower()
    ])
    
    # Aplicamos la funci√≥n de correcci√≥n personalizada
    texto_corregido = corregir_palabras(doc)
    
    # Retornamos toda la informaci√≥n procesada
    return {
        "original": texto,
        "lematizado": lematizado,
        "sin_repeticiones": sin_repeticiones,
        "corregido": texto_corregido
    }
# Patrones con RE
# ----------------------
# Expresiones regulares para fechas, dinero y correos electr√≥nicos
PATRON_FECHAS = r"\b(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}[/-]\d{1,2}[/-]\d{1,2})\b"
PATRON_DINERO = r"\b(?:‚Ç¨?\s?\d{1,3}(?:[\.,]\d{3})*(?:[\.,]\d+)?\s?(?:‚Ç¨|euros|USD|\$)|\$\d+(?:\.\d+)?\b)"
PATRON_EMAIL = r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b"

# Funciones para buscar patrones en texto usando re.findall
def encontrar_fechas(texto): return re.findall(PATRON_FECHAS, texto)
"""
    Extrae patrones de fechas del texto usando expresiones regulares.
    
    Args:
        texto (str): Texto a analizar.
    
    Returns:
        list[str]: Lista de fechas encontradas.
    """
def encontrar_dinero(texto): return re.findall(PATRON_DINERO, texto)
"""
    Extrae patrones monetarios (euros, USD) del texto.
    
    Args:
        texto (str): Texto a analizar.
    
    Returns:
        list[str]: Lista de cantidades monetarias.
    """
def encontrar_correos(texto): return re.findall(PATRON_EMAIL, texto)
"""
    Extrae direcciones de email del texto.
    
    Args:
        texto (str): Texto a analizar.
    
    Returns:
        list[str]: Lista de correos electr√≥nicos.
    """

# ----------------------
# Resumen simple
# ----------------------
# Extrae un resumen simple basado en relevancia de oraciones que contienen sustantivos
def resumen_simple(texto, n=3, nlp=None):
    """Genera un resumen extractivo tomando las oraciones m√°s relevantes.

    La funci√≥n ordena las oraciones del texto por un puntaje heur√≠stico que
    considera principalmente la presencia de sustantivos y penaliza la
    longitud de la oraci√≥n. Si se proporciona un objeto `nlp` de spaCy,
    se usa para segmentar en oraciones y para identificar POS (sustantivos).
    Si `nlp` es `None`, la segmentaci√≥n se hace por puntos (`.`) y la
    tokenizaci√≥n se realiza con una expresi√≥n regular.

    Args:
        texto (str): Texto de entrada a resumir. Si es una cadena vac√≠a
            o contiene solo espacios, la funci√≥n devuelve el mensaje
            de error `"Error: texto vac√≠o."`.
        n (int, optional): N√∫mero m√°ximo de oraciones que formar√°n el
            resumen. Valor por defecto: 3.
        nlp (object, optional): Pipeline de spaCy (por ejemplo, `nlp = spacy.load("es_core_news_sm")`).
            Si se proporciona, se espera que `nlp(texto)` devuelva un objeto
            con atributo `.sents` para iterar sentencias y que cada token
            tenga `.pos_` y `.text`. Si no se proporciona, se usan
            reglas simples basadas en `split('.')` y `re.findall`.

    Returns:
        str: Resumen formado por hasta `n` oraciones relevantes ‚Äîlas oraciones
        seleccionadas se devuelven en el orden original del texto‚Äî.
        Si `texto` est√° vac√≠o, devuelve la cadena `"Error: texto vac√≠o."`.
        Si el n√∫mero de oraciones del texto es menor o igual a `n`, se
        retorna el texto completo sin cambios.

    Raises:
        Ninguna excepci√≥n es lanzada expl√≠citamente por la funci√≥n; en su lugar
        devuelve mensajes de error como cadenas (p. ej. para texto vac√≠o).
        Nota: si se proporciona `nlp` incorrecto, pueden surgir errores desde
        la propia librer√≠a spaCy.

    Ejemplos:
        >>> resumen_simplee("Esta es la primera. Segunda importante. Tercera.", n=2)
        'Esta es la primera. Segunda mportain)e.'
        >>> # Usando spaCy (ejemplo conceptual)
       >>> # nlp = spacy.load("es_core_news_sm")"
   R    >>> # resumen_simple(texto_largo, n=3, nlp=nlp)

    Notas:
        - El m√©todo heur√≠stico es simple y pensado para res√∫menes r√°pidos;
          para mejores resultados se recomienda usar t√©cnicas m√°s avanzadas
          (TF-IDF, TextRank, modelos de lenguaje, etc.).
        - Cuando `nlp` es `None`, la segmentaci√≥n por puntos es rudimentaria
          y puede fallar con abreviaturas o n√∫meros (p. ej. "p.ej.").
    """
    if not texto or not texto.strip():
        return "Error: texto vac√≠o."
    oraciones = list(nlp(texto).sents) if nlp else [s.strip() for s in texto.split('.') if s.strip()]
    if len(oraciones) <= n:
        return texto
    puntuaciones = []
    for i, oracion in enumerate(oraciones):
        puntaje = 0
        tokens = oracion if nlp else re.findall(r"\w+", oracion)
        if nlp:
            sustantivos = [t for t in oracion if t.pos_ == "NOUN"]
            puntaje += len(sustantivos)
            longitud = len(oracion.text)
        else:
            sustantivos = [w for w in tokens if len(w) > 2]
            puntaje += len(sustantivos)
            longitud = len(oracion)
        puntaje -= longitud / 200.0
        if i == 0:
            puntaje += 1
        puntuaciones.append((i, puntaje))
    mejores = sorted(puntuaciones, key=lambda x: x[1], reverse=True)[:n]
    indices = sorted([idx for idx, _ in mejores])
    return " ".join(oraciones[i].text.strip() if nlp else oraciones[i] for i in indices)


# ----------------------
# Extracci√≥n NER
# ----------------------
# Extrae entidades nombradas del texto con spaCy, clasificando en categor√≠as relevantes
def extraer_entidades(texto, nlp):
    """
    Extrae entidades nombradas (NER) clasificadas por tipo.
    
    Args:
        texto (str): Texto a analizar.
        nlp: Pipeline spaCy.
    
    Returns:
        dict: {'Personas': [...], 'Lugares': [...], ...}
    """
    if nlp is None:
        print("Aviso: spaCy no disponible ‚Äî NER no puede ejecutarse.")
        return {}
    doc = nlp(texto)
    def extraer(tipo): return [ent.text for ent in doc.ents if ent.label_ == tipo]
    return {
        'Personas': sorted(set(extraer('PER'))),
        'Lugares': sorted(set(extraer('LOC'))),
        'Empresas': sorted(set(extraer('ORG'))),
        'Fechas': sorted(set(extraer('DATE'))),
        'Cantidades': sorted(set([ent.text for ent in doc.ents if ent.label_ == 'QUANTITY']))
    }

# ----------------------
# Palabras clave - Marius
# ----------------------
# Extrae palabras clave usando nltk para filtrar stopwords y spaCy para sustantivos y verbos
def extraer_palabras_clave(texto, nlp=None):
    """
        Extrae palabras clave relevantes de un texto en espa√±ol.

        Proceso:
        - Usa NLTK para tokenizar y calcular las `top_5_palabras` (eliminando
            stopwords en espa√±ol y tokens no alfanum√©ricos).
        - Si se proporciona un objeto `nlp` de spaCy, extrae los sustantivos
            y verbos principales (parte del discurso POS) y devuelve sus
            frecuencias.

        Par√°metros:
        - texto (str): texto de entrada. Si est√° vac√≠o, devuelve `None`.
        - nlp (spaCy Language, opcional): objeto spaCy para an√°lisis morfosint√°ctico.

        Retorna:
        dict con claves:
            - 'top_5_palabras': lista de tuplas (palabra, frecuencia)
            - 'sustantivos': lista de tuplas (sustantivo, frecuencia)
            - 'verbos': lista de tuplas (verbo, frecuencia)

        Nota:
        - Si NLTK o spaCy no est√°n disponibles, la funci√≥n hace un "fallback"
            devolviendo listas vac√≠as o usando solo la parte que s√≠ est√© disponible.
        """
    if not texto or not texto.strip():
        return None
    tokens_filtrados, sustantivos_relevantes, verbos_principales = [], [], []
    if nltk:
        tokens = word_tokenize(texto.lower())
        stopwords_es = set(stopwords.words('spanish'))
        tokens_filtrados = [t for t in tokens if t.isalnum() and t not in stopwords_es and len(t) > 2]
        top_5 = Counter(tokens_filtrados).most_common(5)
    else:
        top_5 = []
    if nlp:
        doc = nlp(texto)
        sustantivos_relevantes = Counter([t.text for t in doc if t.pos_ == 'NOUN']).most_common(5)
        verbos_principales = Counter([t.text for t in doc if t.pos_ == 'VERB']).most_common(5)
    return {'top_5_palabras': top_5, 'sustantivos': sustantivos_relevantes, 'verbos': verbos_principales}


# ----------------------
# Sentimiento
# ----------------------
# Determina sentimiento del texto como positivo, neutral o negativo con modelo transformers
def sentimiento_es(texto, clasificador):
    """
    Analiza el sentimiento usando el modelo multilingual-uncased de Nlptown.

    Retorna:
        - sentimiento: "Positivo", "Negativo", "Neutral" o "Error"
        - score: confianza (0‚Äì1)
        - etiqueta_raw: etiqueta original del modelo (ej. "4 stars")
        - valor_continuo: valor normalizado (-1 a 1)
    """
    if not texto or not texto.strip():
        return "Error: texto vac√≠o.", 0.0, ""
    if clasificador is None:
        return "Error: transformers no instalado.", 0.0, "transformers missing"
    try:
        resultado = clasificador(texto)[0]
        etiqueta = resultado.get('label', '')
        puntuacion = resultado.get('score', 0.0)
        if "1" in etiqueta or "2" in etiqueta:
            sentimiento = "Negativo"
        elif "3" in etiqueta:
            sentimiento = "Neutral"
        else:
            sentimiento = "Positivo"
        return sentimiento, puntuacion, etiqueta
    except Exception as e:
        return "Error", 0.0, str(e)
    
